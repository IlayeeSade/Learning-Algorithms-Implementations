{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "gKLybB9YOHHG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DTYPE = torch.float32"
      ],
      "metadata": {
        "id": "uWP2a0UFSocm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UOCOFC3GN0mn"
      },
      "outputs": [],
      "source": [
        "class Flatten(nn.Module):\n",
        "  def __init__(super):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x.view(x.size(0), -1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchNorm1d(nn.Module):\n",
        "  def __init__(self, in_channels, eps=1e-05, momentum=0.1):\n",
        "    super().__init__()\n",
        "    self.gamma = torch.ones(in_channels)\n",
        "    self.beta = torch.zeros(in_channels)\n",
        "    self.training = True\n",
        "    self.momentum = momentum\n",
        "    self.eps = eps\n",
        "    self.running_mean = torch.zeros(in_channels)\n",
        "    self.running_var = torch.ones(in_channels)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # INPUT SHAPE (BATCH_SIZE=M, IN_CHANNELS=C, LENGTH=L)\n",
        "    if self.training:\n",
        "      mean = torch.mean(x, dim=(0, 2), dtype=DTYPE).unsqueeze(0).unsqueeze(2) # (C)\n",
        "      var = torch.var(x, dim=(0,2)).unsqueeze(0).unsqueeze(2) # (C)\n",
        "      self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean\n",
        "      self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var\n",
        "    else:\n",
        "      mean = self.running_mean\n",
        "      var = self.running_var\n",
        "\n",
        "    return ((x - mean) / torch.sqrt(var + self.eps)) * self.gamma.unsqueeze(0).unsqueeze(2) + self.beta.unsqueeze(0).unsqueeze(2)\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]"
      ],
      "metadata": {
        "id": "VLS3DWH-OMpX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEQ_LEN = 1"
      ],
      "metadata": {
        "id": "_d_G1NrPgh9A"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, in_channels, head_size, dv):\n",
        "    super().__init__()\n",
        "    self.head_size = head_size\n",
        "    self.Wk = nn.Linear(in_channels, head_size) # (C, H)\n",
        "    self.Wq = nn.Linear(in_channels, head_size) # (C, H)\n",
        "    self.Wv = nn.Linear(in_channels, dv) # (C, H)\n",
        "\n",
        "    # Register the triangular mask as a buffer\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(SEQ_LEN, SEQ_LEN)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x - (M, C, L)\n",
        "    x = x.transpose(-2,-1)\n",
        "    K = self.Wk(x) # (M, L, H)\n",
        "    Q = self.Wq(x) # (M, L, H)\n",
        "    V = self.Wv(x) # (M, L, H)\n",
        "    scores = Q @ K.transpose(-2, -1) / torch.sqrt(torch.tensor(self.head_size, dtype=DTYPE)) # (M, L, L)\n",
        "\n",
        "    # Create a triangular mask\n",
        "    mask = self.tril[:scores.size(-2), :scores.size(-1)]\n",
        "    scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "    attn_weights = torch.softmax(scores, dim=-1)\n",
        "    return attn_weights @ V\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.Wk, self.Wq, self.Wv]"
      ],
      "metadata": {
        "id": "Vijb47URa-tG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, in_channels, n_heads, head_size, dv):\n",
        "    super().__init__()\n",
        "    self.head_size = head_size\n",
        "    self.heads = [Attention(in_channels, head_size, dv) for i in range(n_heads)]\n",
        "    self.Wo = nn.Linear(dv * n_heads, in_channels)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x - (M, C, L)\n",
        "    self.heads_output = [head(x) for head in self.heads]\n",
        "    return self.Wo(torch.cat(self.heads_output, dim=-1))"
      ],
      "metadata": {
        "id": "dpsGzeg5hSpr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_BATCH_SIZE, MAX_SEQ_LEN, N_KV_HEADS, HEAD_DIM = 1,1,1,1"
      ],
      "metadata": {
        "id": "6gkgDIXqmjO5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GroupedQueryAttention(nn.Module):\n",
        "    def __init__(self, in_channels, head_size, n_heads, n_kv_heads):\n",
        "      self.n_heads = n_heads\n",
        "      self.head_size = head_size\n",
        "      self.n_kv_heads = n_kv_heads\n",
        "      super().__init__()\n",
        "      self.Wq = nn.Linear(in_channels, n_heads * head_size)\n",
        "      self.Wk = nn.Linear(in_channels, n_kv_heads * head_size)\n",
        "      self.Wv = nn.Linear(in_channels, n_kv_heads * head_size)\n",
        "      self.Wo = nn.Linear(n_heads * head_size, in_channels)\n",
        "\n",
        "      # Create empty caches for keys and values.\n",
        "      self.cache_k = torch.zeros((MAX_BATCH_SIZE,MAX_SEQ_LEN,n_kv_heads,head_size))\n",
        "      self.cache_v = torch.zeros((MAX_BATCH_SIZE,MAX_SEQ_LEN,n_kv_heads,head_size))\n",
        "      # Register the triangular mask as a buffer\n",
        "      self.register_buffer('tril', torch.tril(torch.ones(MAX_SEQ_LEN, MAX_SEQ_LEN)))\n",
        "\n",
        "    def forward(self, x, start_pos, mask):\n",
        "        M, C, L = x.shape\n",
        "        x = x.tranpose(-2, -1)\n",
        "        Q, K, V = self.Wq(x), self.Wk(x), self.Wv(x)\n",
        "\n",
        "        Q = Q.view(M, L, self.n_heads, self.head_size)\n",
        "        K = K.view(M, L, self.n_kv_heads, self.head_size)\n",
        "        V = V.view(M, L, self.n_kv_heads, self.head_size)\n",
        "\n",
        "        # Simple Caching\n",
        "        self.cache_k = self.cache_k.to(Q.device)\n",
        "        self.cache_v = self.cache_v.to(Q.device)\n",
        "        self.cache_k[:M, start_pos : start_pos + L] = K\n",
        "        self.cache_v[:M, start_pos : start_pos + L] = V\n",
        "        K = self.cache_k[:M, : start_pos + L]\n",
        "        V = self.cache_v[:M, : start_pos + L]\n",
        "\n",
        "        n_duplicates = self.n_heads / self.n_kv_heads\n",
        "\n",
        "        K = torch.repeat_interleave(K, dim=2, repeats=n_duplicates)\n",
        "        V = torch.repeat_interleave(V, dim=2, repeats=n_duplicates)\n",
        "\n",
        "        Q = Q.transpose(-3 -2) # (M, L, n_heads, head_size) -> (M, n_heads, L, head_size)\n",
        "        K = K.transpose(-3 -2) # (M, L, n_heads, head_size) -> (M, n_heads, L, head_size)\n",
        "        V = V.transpose(-3, -2) # (M, L, n_heads, head_size) -> (M, n_heads, L, head_size)\n",
        "\n",
        "        out = F.scaled_dot_product_attention(Q, K, V, attn_mask=self.tril[:L, :L])\n",
        "        return self.Wo(out.transpose(-3, -2).contiguous().view(M, L, -1))"
      ],
      "metadata": {
        "id": "VpSEOXdPjvSi"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}